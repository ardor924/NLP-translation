{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================\n",
    "# ▶모듈불러오기\n",
    "#================================================================\n",
    "# 시스템\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 데이터분석\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 평가지표\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "# 데이터분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 토치\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 모델\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaForTokenClassification,RobertaTokenizerFast\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "\n",
    "# 기타\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# ▶ 전역설정\n",
    "#===============================================================================\n",
    "TOKEN = '---COPY YOUR TOKEN---'\n",
    "MODEL_NAME = \"EbanLee/kobart-summary-v3\"\n",
    "\n",
    "MODEL = 'kobart'\n",
    "VERSION = 'STEP-0'\n",
    "\n",
    "MAX_LEN = 512\n",
    "# MAX_LEN = 128\n",
    "# MAX_LEN = 250 \n",
    "BATCH_SIZE =4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 5e-5\n",
    "# LEARNING_RATE = 1e-3\n",
    "SEED = 2025\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #===============================================================================\n",
    "# # ▶ 허깅 페이스 로그인\n",
    "# #===============================================================================\n",
    "# from huggingface_hub import login\n",
    "# # TOKEN = open('TOKEN.TXT').read().strip()\n",
    "# login(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "#===============================================================================\n",
    "# ▶ 모델 & 토크나이저 불러오기\n",
    "#===============================================================================\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# ▶ 토크나이저 세팅\n",
    "#===============================================================================\n",
    "input_text = \"\"\"Abstract\n",
    "We introduce a new language representation model called BERT, which stands for\n",
    "Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\n",
    "\"\"\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# 요약 모델 파라미터 설정 \n",
    "#============================================================================= \n",
    "summary_text_ids = \\\n",
    "model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                bos_token_id=model.config.bos_token_id,\n",
    "                eos_token_id=model.config.eos_token_id,\n",
    "                length_penalty=1.0,\n",
    "                max_length=300,\n",
    "                min_length=12,\n",
    "                num_beams=6,\n",
    "                repetition_penalty=1.5,\n",
    "                no_repeat_ngram_size=15,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문\n",
      "Abstract\n",
      "We introduce a new language representation model called BERT, which stands for\n",
      "Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\n",
      "\n",
      "원문길이 :390\n",
      "요약문\n",
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.\n",
      "요약문길이 :135\n"
     ]
    }
   ],
   "source": [
    "#=============================================================================\n",
    "# 결과확인 \n",
    "#============================================================================= \n",
    "summary_text = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)\n",
    "print(f'원문\\n{input_text}')\n",
    "print(f'원문길이 :{len(input_text)}')\n",
    "print(f'요약문\\n{summary_text}')\n",
    "print(f'요약문길이 :{len(summary_text)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
